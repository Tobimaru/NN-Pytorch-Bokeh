{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D GAN distribution sampling\n",
    "\n",
    "This notebook contains a PyTorch implementation of a classic one-dimensional GAN example. It's a relatively simple problem in which the generator tries to mimic a Gaussian distribution. The generator is fed by sampling a noise distribution while the discriminator tries to distinguish between samples drawn from the Gaussian distribution and samples constructed by the generator using noise samples as input.\n",
    "\n",
    "Several implementations have been proposed on the internet, and the goal is to illustrate impact of different models and the evolution of GAN training techniques (still work in progress, so far only a vanilla GAN has been implemented).\n",
    "More detailed descriptions of the problem can be found in the following blogs:\n",
    "\n",
    "http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/\n",
    "\n",
    "https://blog.evjang.com/2016/06/generative-adversarial-nets-in.html\n",
    "\n",
    "https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import output_notebook, figure, show\n",
    "from bokeh.layouts import row\n",
    "from bokeh.io import push_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#embed figures in the notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_real(n, sigma=1.0, mean=-1.0):\n",
    "    return np.sort(np.random.normal(mean, sigma, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_noise(n, bound=5.0):\n",
    "    #use stratified sampling to ensure the mapping maintains ordering \n",
    "    # (see blog for details)\n",
    "    return np.linspace(-bound, bound, n) + \\\n",
    "            np.random.random(n) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    def __init__(self, real_sigma, real_mean, \n",
    "                 noise_range, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        #initialize data providers\n",
    "        self.real_data = RealData(self.batch_size, \n",
    "                                  real_sigma, real_mean)\n",
    "        self.fake_data = FakeData(self.batch_size, noise_range)\n",
    "        self.real_inputs = DataLoader(\n",
    "            self.real_data, batch_size=self.batch_size, \n",
    "            shuffle=False, num_workers=1)\n",
    "        self.fake_inputs = DataLoader(\n",
    "            self.fake_data, batch_size=self.batch_size, \n",
    "            shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap sampling in PyTorch Datasets\n",
    "\n",
    "to use PyTorch sampling functionalities when training the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RealData(Dataset):\n",
    "    def __init__(self, n, sigma, mean):\n",
    "        self.data_size = n\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "    \n",
    "    def sample(self):\n",
    "        self.x = torch.FloatTensor(sample_real(\n",
    "            self.data_size, self.sigma, self.mean)).unsqueeze(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FakeData(Dataset):\n",
    "    def __init__(self, n, bound):\n",
    "        self.data_size = n\n",
    "        self.bound = bound\n",
    "        \n",
    "    def sample(self):\n",
    "        self.x = torch.FloatTensor(sample_noise(\n",
    "            self.data_size, self.bound)).unsqueeze(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we define base classes to collect the common functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelBase, self).__init__()\n",
    "        \n",
    "    def _init_layers(self):\n",
    "        for layer in self.fc_layers:\n",
    "            nn.init.normal(layer.weight.data)\n",
    "            nn.init.constant(layer.bias.data, 0.0)\n",
    "    \n",
    "    def _forward_first(self, x):\n",
    "        for fc, activ in zip(self.fc_layers, self.activations):\n",
    "            x = activ(fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(ModelBase):\n",
    "    def __init__(self, wasserstein=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        if wasserstein:\n",
    "            self.forward = self._forward_wasserstein\n",
    "        else:\n",
    "            self.last_activation = nn.Sigmoid()\n",
    "            self.forward = self._forward_vanilla\n",
    "    \n",
    "    def _forward_vanilla(self, x):\n",
    "        return self.last_activation(\n",
    "            self.fc_layers[-1](self._forward_first(x)))\n",
    "    \n",
    "    def _forward_wasserstein(self, x):\n",
    "        return self.fc_layers[-1](self._forward_first(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks as proposed in http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator1(ModelBase):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Generator1, self).__init__()\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim)] +\n",
    "            [nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.Softplus()])\n",
    "        \n",
    "        self._init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc_layers[-1](self._forward_first(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator1(Discriminator):\n",
    "    def __init__(self, input_dim, hidden_dim, wasserstein=False):\n",
    "        super(Discriminator1, self).__init__(wasserstein)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim)] + \n",
    "            [nn.Linear(hidden_dim, hidden_dim) for i in range(2)] +\n",
    "            [nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.ReLU() for i in range(3)])\n",
    "        \n",
    "        self._init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks as proposed in \n",
    "https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator2(ModelBase):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Generator2, self).__init__()\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim), \n",
    "             nn.Linear(hidden_dim, hidden_dim),\n",
    "             nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.ELU(), nn.Sigmoid()])\n",
    "        \n",
    "        self._init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc_layers[-1](self._forward_first(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator2(Discriminator):\n",
    "    def __init__(self, input_dim, hidden_dim, wasserstein=False):\n",
    "        super(Discriminator2, self).__init__(wasserstein)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim), \n",
    "             nn.Linear(hidden_dim, hidden_dim),\n",
    "             nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.ELU() for i in range(2)])\n",
    "        \n",
    "        self._init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GAN\n",
    "\n",
    "This combines sampler, generator and discriminator, and defines the loss function and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, sampler, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        \n",
    "        self.sampler = sampler\n",
    "        \n",
    "        # initialize labels used for training\n",
    "        self.real_labels = torch.ones(self.sampler.batch_size,1)\n",
    "        self.fake_labels = torch.zeros(self.sampler.batch_size,1)\n",
    "        \n",
    "        # construct models\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator      \n",
    "        self.criterion = nn.BCELoss() # binary cross entropy loss\n",
    "        self.optim_gen = optim.Adam(\n",
    "            self.generator.parameters(), lr=0.001)\n",
    "        self.optim_discrim = optim.Adam(\n",
    "            self.discriminator.parameters(), lr=0.001)\n",
    "        \n",
    "        # for logging\n",
    "        self.discrim_loss = {'real': None, 'fake': None}\n",
    "        self.gen_loss = None\n",
    "    \n",
    "    def learn(self):\n",
    "        # GAN learning without resampling for the generator step\n",
    "        \n",
    "        self.sampler.real_data.sample()\n",
    "        self.sampler.fake_data.sample()\n",
    "        \n",
    "        # iterate over minibatches (in this case only one)\n",
    "        for real_x, fake_x in \\\n",
    "            zip(self.sampler.real_inputs, self.sampler.fake_inputs):\n",
    "            \n",
    "            self.discrim_loss['real'], self.discrim_loss['fake'] = \\\n",
    "                self._step_discriminator(real_x, fake_x)\n",
    "            self.gen_loss = self._step_generator(fake_x)\n",
    "                             \n",
    "    def learn_discriminator(self):\n",
    "        \n",
    "        self.sampler.real_data.sample()\n",
    "        self.sampler.fake_data.sample()\n",
    "        \n",
    "        for real_x, fake_x in \\\n",
    "            zip(self.sampler.real_inputs, self.sampler.fake_inputs):\n",
    "        \n",
    "            self.discrim_loss['real'], self.discrim_loss['fake'] = \\\n",
    "                    self._step_discriminator(real_x, fake_x)\n",
    "            \n",
    "    def learn_generator(self):\n",
    "        \n",
    "        self.sampler.fake_data.sample()\n",
    "        \n",
    "        for x in self.sampler.fake_inputs:\n",
    "            self.gen_loss = self._step_generator(x)\n",
    "            \n",
    "    def _step_discriminator(self, real_x, fake_x):\n",
    "        \n",
    "        # Learn discriminator (keep generator fixed).\n",
    "        # We want to compute the loss = \n",
    "        # -log(discrim(real_x)) - log(1 - discrim(gen(fake_x)).\n",
    "        # To do so, both terms can be computed separately,\n",
    "        # because the gradients are added together\n",
    "        # when calling backward()\n",
    "        \n",
    "        self.discriminator.zero_grad()\n",
    "        output = self.discriminator(Variable(real_x))\n",
    "        loss_real = self.criterion(output, Variable(self.real_labels))\n",
    "        loss_real.backward()\n",
    "\n",
    "        forged = self.generator(Variable(fake_x))\n",
    "        # call detach because we don't need to update\n",
    "        # the gradients for the generator (parameters are \n",
    "        # not trained in this step)\n",
    "        output = self.discriminator(forged.detach())\n",
    "        loss_fake = self.criterion(output, Variable(self.fake_labels))\n",
    "        loss_fake.backward()\n",
    "        \n",
    "        self.optim_discrim.step()\n",
    "        \n",
    "        return loss_real.data[0], loss_fake.data[0]\n",
    "    \n",
    "    def _step_generator(self, x):\n",
    "    \n",
    "        # Learn generator (keep discriminator fixed).\n",
    "        \n",
    "        self.generator.zero_grad()\n",
    "        output = self.discriminator(self.generator(Variable(x)))\n",
    "        # TODO: Not sure here if gradients are computed for the discriminator,\n",
    "        # and if so, how to prevent that. \n",
    "        loss = self.criterion(output, Variable(self.real_labels))\n",
    "        loss.backward()\n",
    "        self.optim_gen.step()\n",
    "        \n",
    "        return loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging and plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging of losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_discriminator(loss_real, loss_fake):\n",
    "    print('discriminator | '\n",
    "          '{:.4f} | {:.4f} | {:.4f}'.format(\n",
    "              loss_real, loss_fake, loss_real + loss_fake)) \n",
    "    \n",
    "def log_generator(loss):\n",
    "    print('generator | {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the decision boundary, the real Gaussian distribution, and the forged distribution as generated by the generator in a figure. The evolution of the losses will be plotted in a second figure. The following functions construct data structures to store the data needed to update the figures in bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_real_hist(numb_points, numb_bins, sampler):\n",
    "    \n",
    "    mean = sampler.real_data.mean\n",
    "    sigma = sampler.real_data.sigma\n",
    "    sample_range = sampler.fake_data.bound\n",
    "    \n",
    "    dat = {}\n",
    "    \n",
    "    samples_real = sample_real(numb_points, sigma, mean)\n",
    "    \n",
    "    hist_bins = np.linspace(-sample_range, sample_range, numb_bins)\n",
    "    dat['hist_real'], _ = np.histogram(samples_real, \n",
    "                                       bins=hist_bins, density=True) \n",
    "    dat['x'] = np.linspace(-sample_range, sample_range, numb_bins - 1)\n",
    "    \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_gan_hist(numb_points, numb_bins, gan):\n",
    "    \n",
    "    sample_range = gan.sampler.fake_data.bound\n",
    "    \n",
    "    dat = {}\n",
    "    \n",
    "    # fake distribution\n",
    "    gen_x = torch.FloatTensor(np.linspace(\n",
    "        -sample_range, sample_range, numb_points)).unsqueeze(-1)\n",
    "    samples_fake = gan.generator(Variable(gen_x)).data.numpy().squeeze(-1)\n",
    "    hist_bins = np.linspace(-sample_range, sample_range, numb_bins)\n",
    "    dat['hist_fake'], _ = np.histogram(samples_fake, \n",
    "                                       bins=hist_bins, density=True)\n",
    "    \n",
    "    # decision boundary (numb_bins - 1 to get same length as \n",
    "    # fake distribution data, otherwise bokeh is complaining\n",
    "    xs = np.linspace(-sample_range, sample_range, numb_bins - 1)\n",
    "    discr_x = torch.FloatTensor(xs).unsqueeze(-1)\n",
    "    dat['labels'] = gan.discriminator(\n",
    "        Variable(discr_x)).data.numpy().squeeze(-1) \n",
    "    \n",
    "    dat['x'] = xs\n",
    "    \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_data_gan_loss(gan, old_dat, epoch):\n",
    "    \n",
    "    dat = {}\n",
    "    dat['x'] = np.append(old_dat['x'], [epoch]) \\\n",
    "               if epoch else np.array([epoch])\n",
    "    dloss_real = gan.discrim_loss['real']\n",
    "    dloss_fake = gan.discrim_loss['fake']\n",
    "    dat['loss_discrim_real'] = np.append(old_dat['loss_discrim_real'], \n",
    "                                  [dloss_real]) \\\n",
    "                               if epoch else np.array([dloss_real])\n",
    "    dat['loss_discrim_fake'] = np.append(old_dat['loss_discrim_fake'], \n",
    "                                  [dloss_fake]) \\\n",
    "                               if epoch else np.array([dloss_fake])\n",
    "    dat['loss_discrim_total'] = np.append(old_dat['loss_discrim_total'], \n",
    "                                  [dloss_real + dloss_fake]) \\\n",
    "                                if epoch else np.array([dloss_real + dloss_fake])\n",
    "    dat['loss_generator'] = np.append(old_dat['loss_generator'], \n",
    "                                      [gan.gen_loss]) \\\n",
    "                            if epoch else np.array([gan.gen_loss])\n",
    "    \n",
    "    return dat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function displays the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_data(data_real_hist, data_gan_hist, data_gan_loss, bound):\n",
    "    \n",
    "    lw = 2\n",
    "    \n",
    "    fig_hist = figure(plot_width=350, plot_height=350,\n",
    "                      x_range=(-bound, bound))\n",
    "\n",
    "    fig_hist.line('x', 'hist_real', source=data_real_hist,\n",
    "             line_width=lw, line_color='blue', legend='real distribution')\n",
    "    fig_hist.line('x', 'labels', source=data_gan_hist,\n",
    "             line_width=lw, line_color='green', legend='decision')\n",
    "    fig_hist.line('x', 'hist_fake', source=data_gan_hist, \n",
    "             line_width=lw, line_color='red', legend='fake distribution')\n",
    "    \n",
    "    # unfortunately legends cannot be moved (yet) interactively\n",
    "    fig_hist.legend.location = \"top_left\"\n",
    "    #fig.legend.click_policy=\"hide\"\n",
    "    \n",
    "    fig_loss = figure(plot_width=640, plot_height=350)\n",
    "    fig_loss.line('x', 'loss_discrim_real', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='blue', legend='D-loss real')\n",
    "    fig_loss.line('x', 'loss_discrim_fake', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='green', legend='D-loss fake')\n",
    "    fig_loss.line('x', 'loss_discrim_total', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='red', legend='D-loss total')\n",
    "    fig_loss.line('x', 'loss_generator', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='black', legend='G-loss')\n",
    "    \n",
    "    show(row(fig_hist, fig_loss), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encapsulate training in a function to make it reusable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gan(gan, nb_points, nb_bins, log_step, number_epochs):\n",
    "    data_real = ColumnDataSource(\n",
    "    data=get_data_real_hist(nb_points, nb_bins, sampler))\n",
    "    data_gan_hist = ColumnDataSource(\n",
    "    data={'x': [], 'labels': [], 'hist_fake': []})\n",
    "    # when using empty arrays, y-axis tick labels are not showing\n",
    "    data_gan_loss = ColumnDataSource(\n",
    "        data={'x': [0.], \n",
    "              'loss_discrim_real': [0.],\n",
    "              'loss_discrim_fake': [0.],\n",
    "              'loss_discrim_total': [0.],\n",
    "              'loss_generator': [0.]})\n",
    "\n",
    "    show_data(data_real, data_gan_hist, data_gan_loss, \n",
    "              gan.sampler.fake_data.bound)\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "        #gan.learn()\n",
    "        gan.learn_discriminator()\n",
    "        gan.learn_generator()\n",
    "\n",
    "        if not (epoch % log_step):\n",
    "            data_gan_hist.data = get_data_gan_hist(4000, 100, gan)\n",
    "            data_gan_loss.data = update_data_gan_loss(\n",
    "                gan, data_gan_loss.data, epoch)\n",
    "            push_notebook()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Generator1 and Discriminator1\n",
    "\n",
    "Note that this problem is not stable and can generate various solution, depending on the random seed or on how many times you run the cells. As explained in the blog, the generator will typically converge to a distribution that has similar range but has a more narrow shape. To improve convergence, a pre-training of the decision boundary can be applied as explained in https://blog.evjang.com/2016/06/generative-adversarial-nets-in.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define seed here such that each variant is \n",
    "# tested with same random sequence\n",
    "np.random.seed(42)\n",
    "real_sigma = 0.5\n",
    "real_mean = 4.0\n",
    "noise_range = 8.0\n",
    "hidden_dim = 4\n",
    "batch_size = 8\n",
    "sampler = Sampler(real_sigma, real_mean, noise_range, batch_size)\n",
    "gen1 = Generator1(1, hidden_dim)\n",
    "discrim1 = Discriminator1(1, 2 * hidden_dim)\n",
    "gan1 = GAN(sampler, gen1, discrim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gan1, nb_points=4000, nb_bins=100, log_step=10, number_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Generator2 and Discriminator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define seed here such that each variant is \n",
    "# tested with same random sequence\n",
    "np.random.seed(42)\n",
    "real_sigma = 0.5\n",
    "real_mean = 4.0\n",
    "noise_range = 8.0\n",
    "hidden_dim = 4\n",
    "batch_size = 8\n",
    "sampler = Sampler(real_sigma, real_mean, noise_range, batch_size)\n",
    "gen2 = Generator2(1, hidden_dim)\n",
    "discrim2 = Discriminator2(1, hidden_dim)\n",
    "gan2 = GAN(sampler, gen2, discrim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gan2, nb_points=4000, nb_bins=100, log_step=10, number_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test generator\n",
    "\n",
    "This performs a test to validate the behavior of the generator. It's tested against an implementation of the same model using numpy. It illustrates how weights and bias values in PyTorch layers can be set to evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_numpy():\n",
    "    in_vals = np.matrix([[1. , 2., 3., 4.]])\n",
    "    w1 = np.matrix([[1.], [2.], [3.], [4.]])\n",
    "    w2 = np.matrix([[1., 2., 3., 4.]])\n",
    "    res1 = w1 * in_vals\n",
    "    res2 = np.log(1. + np.exp(res1))\n",
    "    res3 = w2 * res2\n",
    "    return res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dims_test = 4\n",
    "test_weights = np.array([[1.], [2.], [3.], [4.]])\n",
    "weights_gen_fc1 = test_weights\n",
    "bias_gen_fc1 = np.zeros((hidden_dims_test,1))\n",
    "weights_gen_fc2 = test_weights.transpose()\n",
    "bias_gen_fc2 = np.zeros((1,1))\n",
    "\n",
    "gen = Generator(1,hidden_dims_test)\n",
    "gen.fc1.weight.data = torch.FloatTensor(weights_gen_fc1)\n",
    "gen.fc1.bias.data = torch.FloatTensor(bias_gen_fc1)\n",
    "gen.fc2.weight.data = torch.FloatTensor(weights_gen_fc2)\n",
    "gen.fc2.bias.data = torch.FloatTensor(bias_gen_fc2)\n",
    "\n",
    "res = gen(Variable(torch.FloatTensor([[1.], [2.], [3.], [4.]])))\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    res.data.numpy(),                            \n",
    "    generator_numpy().transpose(), decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test discriminator\n",
    "\n",
    "This performs a test of the discriminator model, using baseline values obtained by running equivalent model in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dims_test = 4\n",
    "test_weights = np.array([[0.5, -1.5, 0.33, -0.15]])\n",
    "weights = [test_weights.transpose(), \n",
    "           np.tile(test_weights, (4,1)),\n",
    "           np.tile(test_weights, (4,1)).transpose(), \n",
    "           np.array(np.fliplr(test_weights))] \n",
    "# pytorch does not (yet) support negative stride\n",
    "\n",
    "discrim = Discriminator(1, hidden_dims_test)    \n",
    "for layer, w in zip(discrim.fc_layers, weights):\n",
    "    layer.weight.data = torch.FloatTensor(w)\n",
    "\n",
    "res = discrim(Variable(torch.FloatTensor(\n",
    "    [[1.0],[0.5],[3.0],[0.0]])))\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    res.data.numpy(),\n",
    "    [[0.3061263], [0.3991169], [0.0790827], [0.5]],\n",
    "    decimal=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
