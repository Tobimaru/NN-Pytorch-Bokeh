{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D GAN distribution sampling\n",
    "\n",
    "This notebook contains a PyTorch implementation of a classic one-dimensional GAN example. It's a relatively simple problem in which the generator tries to mimic a Gaussian distribution. The generator is fed by sampling a noise distribution while the discriminator tries to distinguish between samples drawn from the Gaussian distribution and samples constructed by the generator using noise samples as input.\n",
    "\n",
    "Several implementations have been proposed on the internet, and the goal is to illustrate impact of different models and the evolution of Wasserstein GAN (WGAN) training techniques.\n",
    "More detailed descriptions of the problem can be found in the following blogs:\n",
    "\n",
    "http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/\n",
    "\n",
    "https://blog.evjang.com/2016/06/generative-adversarial-nets-in.html\n",
    "\n",
    "https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\n",
    "\n",
    "Relevant publications:\n",
    "\n",
    "GAN:\n",
    "https://arxiv.org/abs/1406.2661\n",
    "\n",
    "Wasserstein GAN:\n",
    "https://arxiv.org/abs/1701.07875\n",
    "\n",
    "Wasserstein GAN with improved training:\n",
    "https://arxiv.org/abs/1704.00028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import output_notebook, figure, show\n",
    "from bokeh.layouts import row\n",
    "from bokeh.io import push_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#embed figures in the notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_real(n, sigma=1.0, mean=-1.0):\n",
    "    return np.sort(np.random.normal(mean, sigma, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_noise(n, bound=5.0):\n",
    "    #use stratified sampling to ensure the mapping maintains ordering \n",
    "    # (see blog for details)\n",
    "    return np.linspace(-bound, bound, n) + \\\n",
    "            np.random.random(n) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used for the Wasserstein-GAN with improved training\n",
    "def sample_epsilon(n):\n",
    "    return np.random.uniform(0.0, 1.0, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap sampling in PyTorch Datasets\n",
    "\n",
    "to use PyTorch sampling functionalities when training the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RealData(Dataset):\n",
    "    def __init__(self, n, sigma, mean):\n",
    "        self.data_size = n\n",
    "        self.sigma = sigma\n",
    "        self.mean = mean\n",
    "    \n",
    "    def sample(self):\n",
    "        self.x = torch.FloatTensor(sample_real(\n",
    "            self.data_size, self.sigma, self.mean)).unsqueeze(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FakeData(Dataset):\n",
    "    def __init__(self, n, bound):\n",
    "        self.data_size = n\n",
    "        self.bound = bound\n",
    "        \n",
    "    def sample(self):\n",
    "        self.x = torch.FloatTensor(sample_noise(\n",
    "            self.data_size, self.bound)).unsqueeze(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    def __init__(self, real_sigma, real_mean, \n",
    "                 noise_range, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        #initialize data providers\n",
    "        self.real_data = RealData(self.batch_size, \n",
    "                                  real_sigma, real_mean)\n",
    "        self.fake_data = FakeData(self.batch_size, noise_range)\n",
    "        self.real_inputs = DataLoader(\n",
    "            self.real_data, batch_size=self.batch_size, \n",
    "            shuffle=False, num_workers=1)\n",
    "        self.fake_inputs = DataLoader(\n",
    "            self.fake_data, batch_size=self.batch_size, \n",
    "            shuffle=False, num_workers=1)\n",
    "        \n",
    "    def sample_epsilon(self):\n",
    "        return torch.FloatTensor(sample_epsilon(\n",
    "            self.batch_size)).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First we define base classes to collect the common functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelBase, self).__init__()\n",
    "        \n",
    "    def _init_layers(self):\n",
    "        for layer in self.fc_layers:\n",
    "            nn.init.normal(layer.weight.data)\n",
    "            nn.init.constant(layer.bias.data, 0.0)\n",
    "    \n",
    "    def _forward_first(self, x):\n",
    "        for fc, activ in zip(self.fc_layers, self.activations):\n",
    "            x = activ(fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(ModelBase):\n",
    "    def __init__(self, wasserstein=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.wasserstein = wasserstein\n",
    "\n",
    "        if wasserstein:\n",
    "            self.forward = self._forward_wasserstein\n",
    "        else:\n",
    "            self.last_activation = nn.Sigmoid()\n",
    "            self.forward = self._forward_vanilla\n",
    "    \n",
    "    def is_wasserstein(self):\n",
    "        return self.wasserstein\n",
    "    \n",
    "    def _forward_vanilla(self, x):\n",
    "        return self.last_activation(\n",
    "            self.fc_layers[-1](self._forward_first(x)))\n",
    "    \n",
    "    def _forward_wasserstein(self, x):\n",
    "        return self.fc_layers[-1](self._forward_first(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks as proposed in http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator1(ModelBase):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Generator1, self).__init__()\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim)] +\n",
    "            [nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.Softplus()])\n",
    "        \n",
    "        self._init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc_layers[-1](self._forward_first(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator1(Discriminator):\n",
    "    def __init__(self, input_dim, hidden_dim, wasserstein=False):\n",
    "        super(Discriminator1, self).__init__(wasserstein)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim)] + \n",
    "            [nn.Linear(hidden_dim, hidden_dim) for i in range(2)] +\n",
    "            [nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.ReLU() for i in range(3)])\n",
    "        \n",
    "        self._init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks as proposed in \n",
    "https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator2(ModelBase):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Generator2, self).__init__()\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim), \n",
    "             nn.Linear(hidden_dim, hidden_dim),\n",
    "             nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.ELU(), nn.Sigmoid()])\n",
    "        \n",
    "        self._init_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc_layers[-1](self._forward_first(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator2(Discriminator):\n",
    "    def __init__(self, input_dim, hidden_dim, wasserstein=False):\n",
    "        super(Discriminator2, self).__init__(wasserstein)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, hidden_dim), \n",
    "             nn.Linear(hidden_dim, hidden_dim),\n",
    "             nn.Linear(hidden_dim, 1)])\n",
    "        \n",
    "        self.activations = nn.ModuleList(\n",
    "            [nn.ELU() for i in range(2)])\n",
    "        \n",
    "        self._init_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks as proposed in https://github.com/kremerj/gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator3(ModelBase):\n",
    "    def __init__(self, input_dim,  hidden_dim):\n",
    "        super(Generator3, self).__init__()\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(1, hidden_dim),\n",
    "                                        nn.Linear(hidden_dim, 1)])\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "        self._init_layers()\n",
    "                                       \n",
    "    def forward(self, x):\n",
    "        return self.fc_layers[1](self.activation(self.fc_layers[0](x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator3(Discriminator):\n",
    "    def __init__(self, input_dim, hidden_dim, wasserstein=False):\n",
    "        super(Discriminator3, self).__init__(wasserstein)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(1, hidden_dim),\n",
    "                                        nn.Linear(hidden_dim, 1)])\n",
    "        self.activations = nn.ModuleList([nn.ReLU()])\n",
    "    \n",
    "        self._init_layers() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GAN\n",
    "\n",
    "This combines sampler, generator and discriminator, and defines the loss function and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, sampler, generator, discriminator, flavor='vanilla'):\n",
    "        super(GAN, self).__init__()\n",
    "        \n",
    "        # store the sampling\n",
    "        self.sampler = sampler\n",
    "        \n",
    "        # store the models\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "        self.flavor = flavor\n",
    "        \n",
    "        # ToDo: use decorators instead of this heavy conditionals\n",
    "        \n",
    "        # define the criterion\n",
    "        if flavor == 'vanilla':\n",
    "            self.number_discrim_iterations = 1\n",
    "            self.real_labels = torch.ones(self.sampler.batch_size,1)\n",
    "            self.fake_labels = torch.zeros(self.sampler.batch_size,1)\n",
    "            self.criterion = nn.BCELoss() # binary cross entropy loss\n",
    "            self._eval_criterion_real = self._eval_criterion_real_vanilla\n",
    "            self._eval_criterion_fake = self._eval_criterion_fake_vanilla\n",
    "            self._loss_penalty = self._no_loss_penalty\n",
    "            self._postprocess_discriminator = \\\n",
    "                self._no_postprocess_discriminator\n",
    "                \n",
    "        if 'wasserstein' in flavor:\n",
    "            self.number_discrim_iterations = 5\n",
    "            self.criterion = torch.mean\n",
    "            self._eval_criterion_real = self._eval_criterion_real_wasserstein\n",
    "            self._eval_criterion_fake = self._eval_criterion_fake_wasserstein\n",
    "           \n",
    "        if flavor == 'wasserstein':\n",
    "            self._loss_penalty = self._no_loss_penalty\n",
    "            self._postprocess_discriminator = \\\n",
    "                self._postprocess_discriminator_wasserstein\n",
    "            \n",
    "        if flavor == 'wasserstein_improved':\n",
    "            self._loss_penalty = self._loss_penalty_wasserstein_improved\n",
    "            self.regularization_lambda = 0.1\n",
    "            self._postprocess_discriminator = \\\n",
    "                self._no_postprocess_discriminator\n",
    "        \n",
    "        # create optimizers\n",
    "        self.optim_gen = optim.Adam(\n",
    "            self.generator.parameters(), lr=0.001, betas=(0.5, 0.9))\n",
    "        self.optim_discrim = optim.Adam(\n",
    "            self.discriminator.parameters(), lr=0.001, betas=(0.5, 0.9))\n",
    "        \n",
    "        # for logging\n",
    "        self.discrim_loss = {'real': None, 'fake': None, 'penalty': None}\n",
    "        self.gen_loss = None\n",
    "    \n",
    "    def _eval_criterion_real_vanilla(self, output):\n",
    "        return self.criterion(output, Variable(self.real_labels))\n",
    "    \n",
    "    def _eval_criterion_fake_vanilla(self, output):\n",
    "        return self.criterion(output, Variable(self.fake_labels))\n",
    "    \n",
    "    def _eval_criterion_real_wasserstein(self, output):\n",
    "        return -self.criterion(output)\n",
    "    \n",
    "    def _eval_criterion_fake_wasserstein(self, output):\n",
    "        return self.criterion(output)\n",
    "    \n",
    "    def _no_postprocess_discriminator(self):\n",
    "        return\n",
    "    \n",
    "    def _postprocess_discriminator_wasserstein(self):\n",
    "        for p in self.discriminator.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "            \n",
    "    def _postprocess_discriminator_wasserstein_improved(self):\n",
    "        return\n",
    "        \n",
    "    def _no_loss_penalty(self, real_x, forged_x):\n",
    "        return None\n",
    "    \n",
    "    def _loss_penalty_wasserstein_improved(self, real_x, forged_x):\n",
    "        \n",
    "        eps = self.sampler.sample_epsilon()\n",
    "        penalty_input = Variable(eps * real_x + (1.0 - eps) * forged_x, \n",
    "                                 requires_grad=True)\n",
    "        \n",
    "        # pytorch can only compute gradients for a scalar output.\n",
    "        # To enable computing gradients for the discriminator,\n",
    "        # we need to give the dummy gradient w.r.t to a scalar output.\n",
    "        output = self.discriminator(penalty_input)\n",
    "        grads = torch.autograd.grad(output, penalty_input, \n",
    "                                    grad_outputs=\n",
    "                                        output.data.new(output.shape).fill_(1),\n",
    "                                    create_graph=True,\n",
    "                                    retain_graph=True,\n",
    "                                    only_inputs=True)[0]\n",
    "        return self.regularization_lambda * torch.mean(\n",
    "                        (torch.norm(grads,2,1) - 1.0) ** 2)\n",
    "        \n",
    "    def learn(self):\n",
    "        # GAN learning without resampling for the generator step\n",
    "        \n",
    "        self.sampler.real_data.sample()\n",
    "        self.sampler.fake_data.sample()\n",
    "        \n",
    "        # iterate over minibatches (in this case only one)\n",
    "        for _ in range(self.number_discrim_iterations):\n",
    "            for real_x, fake_x in \\\n",
    "                zip(self.sampler.real_inputs, self.sampler.fake_inputs):\n",
    "\n",
    "                self.discrim_loss['real'], self.discrim_loss['fake'], \\\n",
    "                self.discrim_loss['penalty'] = \\\n",
    "                    self._step_discriminator(real_x, fake_x)\n",
    "\n",
    "        self.gen_loss = self._step_generator(fake_x)\n",
    "                             \n",
    "    def learn_discriminator(self):\n",
    "        \n",
    "        self.sampler.real_data.sample()\n",
    "        self.sampler.fake_data.sample()\n",
    "        \n",
    "        for _ in range(self.number_discrim_iterations):\n",
    "            for real_x, fake_x in \\\n",
    "                zip(self.sampler.real_inputs, self.sampler.fake_inputs):\n",
    "\n",
    "                self.discrim_loss['real'], self.discrim_loss['fake'], \\\n",
    "                self.discrim_loss['penalty'] = \\\n",
    "                    self._step_discriminator(real_x, fake_x)\n",
    "            \n",
    "    def learn_generator(self):\n",
    "        \n",
    "        self.sampler.fake_data.sample()\n",
    "        \n",
    "        for x in self.sampler.fake_inputs:\n",
    "            self.gen_loss = self._step_generator(x)\n",
    "            \n",
    "    def _step_discriminator(self, real_x, fake_x):\n",
    "        \n",
    "        # Learn discriminator (keep generator fixed).\n",
    "        # We want to compute the loss = \n",
    "        # -log(discrim(real_x)) - log(1 - discrim(gen(fake_x)).\n",
    "        # To do so, both terms can be computed separately,\n",
    "        # because the gradients are added together\n",
    "        # when calling backward()\n",
    "        \n",
    "        self.discriminator.zero_grad()\n",
    "        output = self.discriminator(Variable(real_x))\n",
    "        loss_real = self._eval_criterion_real(output)\n",
    "        loss_real.backward()\n",
    "\n",
    "        forged = self.generator(Variable(fake_x))\n",
    "        # call detach because we don't need to update\n",
    "        # the gradients for the generator (parameters are \n",
    "        # not trained in this step)\n",
    "        output = self.discriminator(forged.detach())\n",
    "        loss_fake = self._eval_criterion_fake(output)\n",
    "        loss_fake.backward()\n",
    "        \n",
    "        loss_penalty = self._loss_penalty(real_x, forged.detach().data)\n",
    "        if loss_penalty is not None:\n",
    "            loss_penalty.backward()\n",
    "            loss_penalty_value = loss_penalty.data[0]\n",
    "        else:\n",
    "            loss_penalty_value = None\n",
    "            \n",
    "        self.optim_discrim.step()\n",
    "        \n",
    "        self._postprocess_discriminator()\n",
    "        \n",
    "        return loss_real.data[0], loss_fake.data[0], loss_penalty_value\n",
    "    \n",
    "    def _step_generator(self, x):\n",
    "    \n",
    "        # Learn generator (keep discriminator fixed).\n",
    "        \n",
    "        self.generator.zero_grad()\n",
    "        output = self.discriminator(self.generator(Variable(x)))\n",
    "        # TODO: Not sure here if gradients are computed for the discriminator,\n",
    "        # and if so, how to prevent that. \n",
    "        loss = self._eval_criterion_real(output)\n",
    "        loss.backward()\n",
    "        self.optim_gen.step()\n",
    "        \n",
    "        return loss.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging and plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging of losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_discriminator(loss_real, loss_fake):\n",
    "    print('discriminator | '\n",
    "          '{:.4f} | {:.4f} | {:.4f}'.format(\n",
    "              loss_real, loss_fake, loss_real + loss_fake)) \n",
    "    \n",
    "def log_generator(loss):\n",
    "    print('generator | {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the decision boundary, the real Gaussian distribution, and the forged distribution as generated by the generator in a figure. The evolution of the losses will be plotted in a second figure. The following functions construct data structures to store the data needed to update the figures in bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_real_hist(numb_points, numb_bins, sampler):\n",
    "    \n",
    "    mean = sampler.real_data.mean\n",
    "    sigma = sampler.real_data.sigma\n",
    "    sample_range = sampler.fake_data.bound\n",
    "    \n",
    "    dat = {}\n",
    "    \n",
    "    samples_real = sample_real(numb_points, sigma, mean)\n",
    "    \n",
    "    hist_bins = np.linspace(-sample_range, sample_range, numb_bins)\n",
    "    dat['hist_real'], _ = np.histogram(samples_real, \n",
    "                                       bins=hist_bins, density=True) \n",
    "    dat['x'] = np.linspace(-sample_range, sample_range, numb_bins - 1)\n",
    "    \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_gan_hist(numb_points, numb_bins, gan):\n",
    "    \n",
    "    sample_range = gan.sampler.fake_data.bound\n",
    "    \n",
    "    dat = {}\n",
    "    \n",
    "    # fake distribution\n",
    "    gen_x = torch.FloatTensor(np.linspace(\n",
    "        -sample_range, sample_range, numb_points)).unsqueeze(-1)\n",
    "    samples_fake = gan.generator(Variable(gen_x)).data.numpy().squeeze(-1)\n",
    "    hist_bins = np.linspace(-sample_range, sample_range, numb_bins)\n",
    "    dat['hist_fake'], _ = np.histogram(samples_fake, \n",
    "                                       bins=hist_bins, density=True)\n",
    "    \n",
    "    # decision boundary (numb_bins - 1) to get same length as \n",
    "    # fake distribution data, otherwise bokeh is complaining\n",
    "    xs = np.linspace(-sample_range, sample_range, numb_bins - 1)\n",
    "    discr_x = torch.FloatTensor(xs).unsqueeze(-1)\n",
    "    decision_bound = gan.discriminator(\n",
    "        Variable(discr_x)).data.numpy().squeeze(-1) \n",
    "    \n",
    "    if gan.discriminator.is_wasserstein():\n",
    "        max_label = np.max(decision_bound)\n",
    "        min_label = np.min(decision_bound)\n",
    "        range_label = max_label - min_label\n",
    "        if abs(range_label) > 1e-5: \n",
    "            decision_bound = (decision_bound - min_label)/(max_label - min_label)\n",
    "    \n",
    "    dat['labels'] = decision_bound\n",
    "    \n",
    "    dat['x'] = xs\n",
    "    \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_data_gan_loss(gan, old_dat, epoch):\n",
    "    \n",
    "    dat = {}\n",
    "    dat['x'] = np.append(old_dat['x'], [epoch]) \\\n",
    "               if epoch else np.array([epoch])\n",
    "    dloss_real = gan.discrim_loss['real']\n",
    "    dloss_fake = gan.discrim_loss['fake'] \n",
    "    dloss_total = dloss_real + dloss_fake\n",
    "    dat['loss_discrim_real'] = np.append(old_dat['loss_discrim_real'], \n",
    "                                  [dloss_real]) \\\n",
    "                               if epoch else np.array([dloss_real])\n",
    "    dat['loss_discrim_fake'] = np.append(old_dat['loss_discrim_fake'], \n",
    "                                  [dloss_fake]) \\\n",
    "                               if epoch else np.array([dloss_fake])\n",
    "    if gan.discriminator.is_wasserstein():\n",
    "        dloss_penalty = gan.discrim_loss['penalty']\n",
    "        dloss_total += dloss_penalty\n",
    "        dat['loss_discrim_penalty'] = np.append(old_dat['loss_discrim_penalty'], \n",
    "                                  [dloss_penalty]) \\\n",
    "                               if epoch else np.array([dloss_penalty])\n",
    "    dat['loss_discrim_total'] = np.append(old_dat['loss_discrim_total'], \n",
    "                                  [dloss_total]) \\\n",
    "                                if epoch else np.array([dloss_total])\n",
    "    dat['loss_generator'] = np.append(old_dat['loss_generator'], \n",
    "                                      [gan.gen_loss]) \\\n",
    "                            if epoch else np.array([gan.gen_loss])\n",
    "    \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function displays the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_data(data_real_hist, data_gan_hist, data_gan_loss, bound):\n",
    "    \n",
    "    lw = 2\n",
    "    \n",
    "    fig_hist = figure(plot_width=350, plot_height=350,\n",
    "                      x_range=(-bound, bound))\n",
    "\n",
    "    fig_hist.line('x', 'hist_real', source=data_real_hist,\n",
    "             line_width=lw, line_color='blue', legend='real distribution')\n",
    "    fig_hist.line('x', 'labels', source=data_gan_hist,\n",
    "             line_width=lw, line_color='green', legend='decision')\n",
    "    fig_hist.line('x', 'hist_fake', source=data_gan_hist, \n",
    "             line_width=lw, line_color='red', legend='fake distribution')\n",
    "    \n",
    "    # unfortunately legends cannot be moved (yet) interactively\n",
    "    fig_hist.legend.location = \"top_left\"\n",
    "    fig_hist.legend.click_policy=\"hide\"\n",
    "    \n",
    "    fig_loss = figure(plot_width=640, plot_height=350)\n",
    "    fig_loss.line('x', 'loss_discrim_real', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='blue', legend='D-loss real')\n",
    "    fig_loss.line('x', 'loss_discrim_fake', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='green', legend='D-loss fake')\n",
    "    if 'loss_discrim_penalty' in data_gan_loss.data:\n",
    "        fig_loss.line('x', 'loss_discrim_penalty', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='magenta', legend='D-loss penalt')\n",
    "    fig_loss.line('x', 'loss_discrim_total', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='red', legend='D-loss total')\n",
    "    fig_loss.line('x', 'loss_generator', source=data_gan_loss, \n",
    "                  line_width=lw, line_color='black', legend='G-loss')\n",
    "    \n",
    "    show(row(fig_hist, fig_loss), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encapsulate training in a function to make it reusable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gan(gan, nb_points, nb_bins, log_step, number_epochs):\n",
    "    data_real = ColumnDataSource(\n",
    "    data=get_data_real_hist(nb_points, nb_bins, sampler))\n",
    "    data_gan_hist = ColumnDataSource(\n",
    "    data={'x': [], 'labels': [], 'hist_fake': []})\n",
    "    # when using empty arrays, y-axis tick labels are not showing\n",
    "    model_data = {'x': [0.], \n",
    "                  'loss_discrim_real': [0.],\n",
    "                  'loss_discrim_fake': [0.],\n",
    "                  'loss_discrim_total': [0.],\n",
    "                  'loss_generator': [0.]}\n",
    "    if gan.discriminator.is_wasserstein():\n",
    "        model_data['loss_discrim_penalty'] = [0.]\n",
    "    data_gan_loss = ColumnDataSource(data=model_data)\n",
    "\n",
    "    show_data(data_real, data_gan_hist, data_gan_loss, \n",
    "              gan.sampler.fake_data.bound)\n",
    "\n",
    "    for epoch in range(number_epochs):\n",
    "        #gan.learn()\n",
    "        gan.learn_discriminator()\n",
    "        gan.learn_generator()\n",
    "\n",
    "        if not (epoch % log_step):\n",
    "            #log_discriminator(gan.discrim_loss['real'], gan.discrim_loss['fake'])\n",
    "            #log_generator(gan.gen_loss)\n",
    "            data_gan_hist.data = get_data_gan_hist(4000, 100, gan)\n",
    "            data_gan_loss.data = update_data_gan_loss(\n",
    "                gan, data_gan_loss.data, epoch)\n",
    "            push_notebook()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Generator1 and Discriminator1\n",
    "\n",
    "Note that this problem is not stable and can generate various solution, depending on the random seed or on how many times you run the cells. As explained in the blog, the generator will typically converge to a distribution that has similar range but has a more narrow shape. To improve convergence, a pre-training of the decision boundary can be applied as explained in https://blog.evjang.com/2016/06/generative-adversarial-nets-in.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define seed here such that each variant is \n",
    "# tested with same random sequence\n",
    "np.random.seed(42)\n",
    "real_sigma = 0.5\n",
    "real_mean = 4.0\n",
    "noise_range = 8.0\n",
    "hidden_dim = 4\n",
    "batch_size = 8\n",
    "sampler = Sampler(real_sigma, real_mean, noise_range, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using vanilla GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen1 = Generator1(1, hidden_dim)\n",
    "discrim1 = Discriminator1(1, 2*hidden_dim)\n",
    "gan1 = GAN(sampler, gen1, discrim1, flavor='vanilla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gan(gan1, nb_points=4000, nb_bins=100, log_step=10, number_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Wasserstein GAN (WGAN)\n",
    "\n",
    "This doesn't seem to work on this example. The discriminator loss function collapses to zero for both the real and fake terms, and the weights in both the generator and discriminator becomes zero. This makes sense, since both the discriminator and the generator loss function becomes zero, if everything is mapped to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen1 = Generator1(1, hidden_dim)\n",
    "discrim1 = Discriminator1(1, 2*hidden_dim, wasserstein=True)\n",
    "gan1 = GAN(sampler, gen1, discrim1, flavor='wasserstein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gan(gan1, nb_points=4000, nb_bins=100, log_step=10, number_epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Wasserstein GAN with improved training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen1 = Generator1(1, hidden_dim)\n",
    "discrim1 = Discriminator1(1, 2*hidden_dim, wasserstein=True)\n",
    "gan1 = GAN(sampler, gen1, discrim1, flavor='wasserstein_improved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gan1, nb_points=4000, nb_bins=100, log_step=10, number_epochs=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Generator2 and Discriminator2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define seed here such that each variant is \n",
    "# tested with same random sequence\n",
    "np.random.seed(42)\n",
    "real_sigma = 0.5\n",
    "real_mean = 4.0\n",
    "noise_range = 8.0\n",
    "hidden_dim = 4\n",
    "batch_size = 8\n",
    "sampler = Sampler(real_sigma, real_mean, noise_range, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using vanilla GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen2 = Generator2(1, hidden_dim)\n",
    "discrim2 = Discriminator2(1, hidden_dim)\n",
    "gan2 = GAN(sampler, gen2, discrim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gan(gan2, nb_points=4000, nb_bins=100, log_step=10, number_epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Wasserstein GAN with improved training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen2 = Generator2(1, hidden_dim)\n",
    "discrim2 = Discriminator2(1, hidden_dim, wasserstein=True)\n",
    "gan2 = GAN(sampler, gen2, discrim2, flavor='wasserstein_improved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gan2, nb_points=4000, nb_bins=100, log_step=10, number_epochs=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Generator3 and Discriminator3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define seed here such that each variant is \n",
    "# tested with same random sequence\n",
    "np.random.seed(42)\n",
    "real_sigma = 0.5\n",
    "real_mean = 4.0\n",
    "noise_range = 8.0\n",
    "hidden_dim = 4\n",
    "batch_size = 64\n",
    "sampler = Sampler(real_sigma, real_mean, noise_range, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Vanilla GAN\n",
    "\n",
    "This doesn't work well. The convergence get stuck, probably because the discriminator is not powerfull enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen3 = Generator3(1, hidden_dim)\n",
    "discrim3 = Discriminator3(1, hidden_dim)\n",
    "gan3 = GAN(sampler, gen3, discrim3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gan3, nb_points=4000, nb_bins=100, log_step=10, number_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen3 = Generator3(1, hidden_dim)\n",
    "discrim3 = Discriminator3(1, hidden_dim, wasserstein=True)\n",
    "gan3 = GAN(sampler, gen3, discrim3, flavor='wasserstein_improved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(gan3, nb_points=4000, nb_bins=100, log_step=10, number_epochs=1450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test generator\n",
    "\n",
    "This performs a test to validate the behavior of the generator. It's tested against an implementation of the same model using numpy. It illustrates how weights and bias values in PyTorch layers can be set to evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_numpy():\n",
    "    in_vals = np.matrix([[1. , 2., 3., 4.]])\n",
    "    w1 = np.matrix([[1.], [2.], [3.], [4.]])\n",
    "    w2 = np.matrix([[1., 2., 3., 4.]])\n",
    "    res1 = w1 * in_vals\n",
    "    res2 = np.log(1. + np.exp(res1))\n",
    "    res3 = w2 * res2\n",
    "    return res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dims_test = 4\n",
    "test_weights = np.array([[1.], [2.], [3.], [4.]])\n",
    "weights_gen_fc1 = test_weights\n",
    "bias_gen_fc1 = np.zeros((hidden_dims_test,1))\n",
    "weights_gen_fc2 = test_weights.transpose()\n",
    "bias_gen_fc2 = np.zeros((1,1))\n",
    "\n",
    "gen = Generator(1,hidden_dims_test)\n",
    "gen.fc1.weight.data = torch.FloatTensor(weights_gen_fc1)\n",
    "gen.fc1.bias.data = torch.FloatTensor(bias_gen_fc1)\n",
    "gen.fc2.weight.data = torch.FloatTensor(weights_gen_fc2)\n",
    "gen.fc2.bias.data = torch.FloatTensor(bias_gen_fc2)\n",
    "\n",
    "res = gen(Variable(torch.FloatTensor([[1.], [2.], [3.], [4.]])))\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    res.data.numpy(),                            \n",
    "    generator_numpy().transpose(), decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test discriminator\n",
    "\n",
    "This performs a test of the discriminator model, using baseline values obtained by running equivalent model in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dims_test = 4\n",
    "test_weights = np.array([[0.5, -1.5, 0.33, -0.15]])\n",
    "weights = [test_weights.transpose(), \n",
    "           np.tile(test_weights, (4,1)),\n",
    "           np.tile(test_weights, (4,1)).transpose(), \n",
    "           np.array(np.fliplr(test_weights))] \n",
    "# pytorch does not (yet) support negative stride\n",
    "\n",
    "discrim = Discriminator(1, hidden_dims_test)    \n",
    "for layer, w in zip(discrim.fc_layers, weights):\n",
    "    layer.weight.data = torch.FloatTensor(w)\n",
    "\n",
    "res = discrim(Variable(torch.FloatTensor(\n",
    "    [[1.0],[0.5],[3.0],[0.0]])))\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    res.data.numpy(),\n",
    "    [[0.3061263], [0.3991169], [0.0790827], [0.5]],\n",
    "    decimal=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class test_grad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test_grad, self).__init__()\n",
    "        \n",
    "        self.fc_layer1 = nn.Linear(2,1)\n",
    "        #self.fc_layer2 = nn.Linear(2,1)\n",
    "        #self.fc_layer1.weight.data = torch.FloatTensor()\n",
    "        self.fc_layer1.weight.data = torch.FloatTensor([[3.0, 7.0]])\n",
    "        #nn.init.constant(self.fc_layer2.weight.data, 3.0)\n",
    "        nn.init.constant(self.fc_layer1.bias.data, 0.0)\n",
    "        #nn.init.constant(self.fc_layer2.bias.data, 0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc_layer1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = test_grad()\n",
    "\n",
    "inp = Variable(torch.FloatTensor([[1.0, 4.0],[2.0, 5.0]]), requires_grad=True)\n",
    "outp = g(inp)\n",
    "gradspred, = torch.autograd.grad(outp, inp, \n",
    "                           grad_outputs=outp.data.new(outp.shape).fill_(1),\n",
    "                           create_graph=True)\n",
    "print(outp)\n",
    "print(gradspred)\n",
    "print((torch.norm(gradspred,2,1) - 1.0) ** 2)\n",
    "\n",
    "print(6.6158 * 6.6158)\n",
    "#print(g.fc_layer1.weight.grad)\n",
    "#print(g.fc_layer2.weight.grad)\n",
    "#print(g.fc_layer1.weight.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
